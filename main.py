import streamlit as st
from pydub import AudioSegment
import threading
import queue
import asyncio
from google.cloud import speech_v2 as speech
from typing import Optional, Callable
import wave
import pyaudio
import time
import io
import os
from dotenv import load_dotenv
from google.api_core.client_options import ClientOptions

load_dotenv()

PROJECT_ID = os.getenv("PROJECT_ID")
LOCATION = os.getenv("LOCATION")
CHIRP_RECOGNIZER_ID = os.getenv("REGONIZER_ID")

class AudioProducer(threading.Thread):
    def __init__(self, wav_data: bytes, chunk_size: int = 1024):
        super().__init__()
        self.wav_data = wav_data
        self.chunk_size = chunk_size
        self.audio_queue = queue.Queue()
        self.is_playing = False
        self.daemon = True
        
    def run(self):
        """Thread's main method - handles audio streaming"""
        # wav_dataë¥¼ BytesIOë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ì½ê¸°
        wav_buffer = io.BytesIO(self.wav_data)
        with wave.open(wav_buffer, 'rb') as wf:
            p = pyaudio.PyAudio()
            
            # Open stream for playback
            stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),
                          channels=wf.getnchannels(),
                          rate=wf.getframerate(),
                          output=True)
            
            self.is_playing = True
            
            try:
                while self.is_playing:
                    data = wf.readframes(self.chunk_size)
                    if not data:
                        break
                        
                    # Play audio
                    stream.write(data)
                    
                    # Put audio data in queue for transcription
                    self.audio_queue.put(data)
                    
                    # Simulate real-time playback speed
                    #time.sleep(self.chunk_size / wf.getframerate())
                    
            finally:
                stream.stop_stream()
                stream.close()
                p.terminate()
                self.is_playing = False
                self.audio_queue.put(None)  # Sentinel value
    
    def stop(self):
        """Stop audio playback and streaming"""
        self.is_playing = False

class TranscriptionConsumer(threading.Thread):
    def __init__(self, 
                 audio_producer: AudioProducer,
                 language_code: str = "ko-KR",
                 on_transcription: Optional[Callable[[str, bool], None]] = None,
                 on_error: Optional[Callable[[Exception], None]] = None):
        super().__init__()
        self.audio_producer = audio_producer
        self.language_code = language_code
        self.on_transcription = on_transcription
        self.on_error = on_error
        self.is_running = False
        self.daemon = True
        self.loop = None

    async def process_audio_stream(self):
        """Process audio stream and handle transcription"""
        client = speech.SpeechAsyncClient(
          client_options=ClientOptions(
            api_endpoint=f"{LOCATION}-speech.googleapis.com",
          )
        )
        
        config = speech.types.RecognitionConfig(
            #encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            #sample_rate_hertz=16000,
            #enable_automatic_punctuation=True,
            auto_decoding_config=speech.types.AutoDetectDecodingConfig(),
            language_codes=["auto"], #[self.language_code],
            model="chirp_2"
        )
        
        streaming_config = speech.types.StreamingRecognitionConfig(
            config=config,
            streaming_features=speech.StreamingRecognitionFeatures(
                interim_results=True
            )
            # interim_results=True
        )

        config_request = speech.types.StreamingRecognizeRequest(
            recognizer=f"projects/{PROJECT_ID}/locations/{LOCATION}/recognizers/_",
            streaming_config=streaming_config
        )
        
        # ë¹„ë™ê¸° ì œë„ˆë ˆì´í„° ì •ì˜
        async def async_request_generator():
            # First request with configuration
            yield config_request
            
            # Process audio chunks from queue
            while self.is_running:
                try:
                    # ë¹„ë™ê¸°ì ìœ¼ë¡œ íì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    chunk = await self.loop.run_in_executor(
                        None, 
                        self.audio_producer.audio_queue.get, 
                        True, 
                        1
                    )
                    #print("Receive Chunk.")
                    #print(chunk)
                    
                    if chunk is None:  # Check for sentinel value
                        print("###################")
                        print("###################")
                        print("Audio stream ended")
                        print("###################")
                        print("###################")
                        break
                    yield speech.types.StreamingRecognizeRequest(audio=chunk)
                except queue.Empty:
                    print("All Queue is Empty.")
                    continue
                except Exception as e:
                    print(e)
                    if self.on_error:
                        await self.loop.run_in_executor(None, self.on_error, e)
                    break
        try:
            print("###################")
            print("start_streaming_recongnition")
            # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì¸ì‹ ì‹¤í–‰
            streaming_response = await client.streaming_recognize(
                requests=async_request_generator()
            )
            
            print("start_streaming_recongnition..2")
            # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì¸ì‹ ì‹¤í–‰
            # ì‘ë‹µ ì²˜ë¦¬
            async for response in streaming_response:
                print("###################")
                print("receive request")

                if not self.is_running:
                    break
                    
                for result in response.results:
                    if not result.alternatives:
                        continue
                        
                    transcript = result.alternatives[0].transcript
                    is_final = result.is_final
                    
                    if self.on_transcription:
                        # ì½œë°±ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
                        await self.loop.run_in_executor(
                            None,
                            self.on_transcription,
                            transcript,
                            is_final
                        )
                        
        except Exception as e:
            print(e)
            if self.on_error:
                await self.loop.run_in_executor(None, self.on_error, e)
        
        print("#####################")
        print("Ended Transcription.")
    
    def run(self):
        """Thread's main method - sets up asyncio event loop"""
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
        
        self.is_running = True
        try:
            self.loop.run_until_complete(self.process_audio_stream())
        finally:
            print("#####################")
            print("Finally ------------------- exited.")
            self.is_running = False
            if self.loop and self.loop.is_running():
                # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  ì‘ì—…ì„ ì •ë¦¬
                pending = asyncio.all_tasks(self.loop)
                self.loop.run_until_complete(asyncio.gather(*pending))
            if self.loop:
                self.loop.close()
    
    async def _stop_async(self):
        """ë¹„ë™ê¸° ì •ë¦¬ ì‘ì—… ìˆ˜í–‰"""
        self.is_running = False
        # í•„ìš”í•œ ì¶”ê°€ ë¹„ë™ê¸° ì •ë¦¬ ì‘ì—… ìˆ˜í–‰
    
    def stop(self):
        """Stop transcription processing"""
        if self.loop and self.loop.is_running():
            self.loop.create_task(self._stop_async())
        self.is_running = False

class AudioTranscriptionManager:
    """Manager class to handle both Producer and Consumer threads"""
    def __init__(self, wav_data: bytes, language_code: str = "ko-KR"):
        self.producer = AudioProducer(wav_data)
        self.consumer = TranscriptionConsumer(
            audio_producer=self.producer,
            language_code=language_code,
            on_transcription=self._handle_transcription,
            on_error=self._handle_error
        )
        
    def _handle_transcription(self, transcript: str, is_final: bool):
        """Handle transcription results"""
        prefix = "Final: " if is_final else "Interim: "
        print(f"{prefix}{transcript}")
    
    def _handle_error(self, error: Exception):
        """Handle errors"""
        print(f"Error occurred: {str(error)}")
    
    def start(self):
        """Start both producer and consumer threads"""
        self.producer.start()
        self.consumer.start()
    
    def stop(self):
        """Stop both producer and consumer threads"""
        self.producer.stop()
        self.consumer.stop()
        
    def join(self):
        """Wait for both threads to complete"""
        self.producer.join()
        self.consumer.join()

# Streamlit ì•±ì—ì„œ ì‚¬ìš©í•  ë•Œì˜ ì˜ˆì‹œ
def create_streamlit_transcription_manager(audio_file, language_code="ko-KR"):
    """Streamlit ì•±ìš© íŠ¸ëœìŠ¤í¬ë¦½ì…˜ ë§¤ë‹ˆì € ìƒì„± í•¨ìˆ˜"""
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ì„ WAVë¡œ ë³€í™˜
    audio = AudioSegment.from_file(audio_file)
    audio = audio.set_channels(1)  # mono
    audio = audio.set_frame_rate(8000)  # 16kHz
    
    # WAV ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
    wav_buffer = io.BytesIO()
    audio.export(wav_buffer, format="wav")
    wav_data = wav_buffer.getvalue()
    
    # ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
    if 'full_transcript' not in st.session_state:
        st.session_state.full_transcript = ""
    
    def update_transcription(transcript: str, is_final: bool):
        if is_final:
            st.session_state.full_transcript += transcript + "\n"
            
        # Streamlitì˜ í…ìŠ¤íŠ¸ ì˜ì—­ ì—…ë°ì´íŠ¸
        transcription_placeholder = st.session_state.get('transcription_placeholder')
        if transcription_placeholder:
            text = st.session_state.full_transcript
            if not is_final:
                text += f"*{transcript}*"
            transcription_placeholder.markdown(text)
    
    def handle_error(error: Exception):
        st.error(f"ì „ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(error)}")
    
    # íŠ¸ëœìŠ¤í¬ë¦½ì…˜ ë§¤ë‹ˆì € ìƒì„±
    manager = AudioTranscriptionManager(wav_data, language_code)
    manager.consumer.on_transcription = update_transcription
    manager.consumer.on_error = handle_error
    
    return manager

import streamlit as st
import time
from typing import Optional
from pydub import AudioSegment

def main():
    st.title("ğŸ¤ ìŒì„± ì „ì‚¬ ì•±")
    
    # Initialize session state variables
    if 'transcription_manager' not in st.session_state:
        st.session_state.transcription_manager = None
    if 'is_transcribing' not in st.session_state:
        st.session_state.is_transcribing = False
    if 'full_transcript' not in st.session_state:
        st.session_state.full_transcript = ""
    
    # Callback functions for buttons
    def start_transcription():
        st.session_state.is_transcribing = True
        
    def stop_transcription():
        st.session_state.is_transcribing = False
        if st.session_state.transcription_manager:
            st.session_state.transcription_manager.stop()
            st.session_state.transcription_manager = None
    
    # Audio file upload
    audio_file = st.file_uploader("ì˜¤ë””ì˜¤ íŒŒì¼ ì„ íƒ (MP3 ë˜ëŠ” AAC)", type=['mp3', 'aac'])
    
    if audio_file:
        st.audio(audio_file)
        
        # Language selection
        language = st.selectbox(
            "ì–¸ì–´ ì„ íƒ",
            options=["í•œêµ­ì–´", "ì˜ì–´", "ì¤‘êµ­ì–´", "ì¼ë³¸ì–´"],
            format_func=lambda x: {
                "í•œêµ­ì–´": "í•œêµ­ì–´ (ko-KR)",
                "ì˜ì–´": "English (en-US)",
                "ì¤‘êµ­ì–´": "ç®€ä½“ä¸­æ–‡ (zh-Hans-CN)",
                "ì¼ë³¸ì–´": "æ—¥æœ¬èª (ja-JP)"
            }[x]
        )
        
        language_codes = {
            "í•œêµ­ì–´": "ko-KR",
            "ì˜ì–´": "en-US",
            "ì¤‘êµ­ì–´": "zh-Hans-CN",
            "ì¼ë³¸ì–´": "ja-JP"
        }
        
        # Control buttons in columns
        col1, col2 = st.columns(2)
        
        with col1:
            start_button = st.button(
                "ë³€í™˜ ì‹œì‘",
                disabled=st.session_state.is_transcribing,
                on_click=start_transcription,
                key='start_button'
            )
            
        with col2:
            stop_button = st.button(
                "ë³€í™˜ ì¤‘ì§€",
                disabled=not st.session_state.is_transcribing,
                on_click=stop_transcription,
                key='stop_button'
            )
        
        # Status indicator and transcription area
        status_container = st.container()
        transcription_container = st.container()
        
        with status_container:
            if st.session_state.is_transcribing:
                st.info("ğŸ”„ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ì¤‘...")
            
        with transcription_container:
            st.markdown("### ë³€í™˜ ê²°ê³¼")
            transcription_placeholder = st.empty()
            st.session_state.transcription_placeholder = transcription_placeholder
            
            if st.session_state.full_transcript:
                transcription_placeholder.markdown(st.session_state.full_transcript)
        
        # Handle transcription process
        if st.session_state.is_transcribing and not st.session_state.transcription_manager:
            try:
                # Create and start transcription manager
                manager = create_streamlit_transcription_manager(
                    audio_file,
                    language_code=language_codes[language]
                )
                st.session_state.transcription_manager = manager
                manager.start()
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                stop_transcription()
        
        # Check completion
        if st.session_state.transcription_manager and not st.session_state.transcription_manager.producer.is_playing:
            stop_transcription()
            st.success("âœ… ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            if st.session_state.full_transcript:
                st.download_button(
                    label="í…ìŠ¤íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    data=st.session_state.full_transcript,
                    file_name=f"transcript_{language_codes[language]}.txt",
                    mime="text/plain"
                )
    
    # Sidebar information
    st.sidebar.markdown("""
    ### ğŸ’¡ ì‚¬ìš© ë°©ë²•
    1. AAC ë˜ëŠ” MP3 í˜•ì‹ì˜ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    2. ìŒì„±ì˜ ì–¸ì–´ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    3. 'ë³€í™˜ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ì‹¤ì‹œê°„ ë³€í™˜ì´ ì‹œì‘ë©ë‹ˆë‹¤.
    4. ì–¸ì œë“  'ë³€í™˜ ì¤‘ì§€' ë²„íŠ¼ìœ¼ë¡œ ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    5. ë³€í™˜ì´ ì™„ë£Œë˜ë©´ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ### âš ï¸ ì£¼ì˜ì‚¬í•­
    - íŒŒì¼ í¬ê¸°ëŠ” 10MB ì´í•˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.
    - ê¹¨ë—í•œ ìŒì„±ì¼ìˆ˜ë¡ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ëª¨ë“  ì˜¤ë””ì˜¤ëŠ” 16kHz ëª¨ë…¸ë¡œ ë³€í™˜ë˜ì–´ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    """
    )

if __name__ == "__main__":
    main()